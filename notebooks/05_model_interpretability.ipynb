{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Model Interpretability\n",
    "\n",
    "This notebook implements model interpretability techniques to explain the predictions of our customer churn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set up plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define file paths\n",
    "ENGINEERED_DATA_PATH = '../data/processed/churn_engineered.csv'\n",
    "MODELS_DIR = '../models/'\n",
    "FEATURE_SETS_PATH = '../models/feature_sets.json'\n",
    "BEST_MODEL_INFO_PATH = '../models/best_model_info.json'\n",
    "PLOTS_DIR = '../docs/plots/'\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load the engineered data\n",
    "df_engineered = pd.read_csv(ENGINEERED_DATA_PATH)\n",
    "print(f\"Engineered dataset shape: {df_engineered.shape}\")\n",
    "\n",
    "# Load feature sets\n",
    "with open(FEATURE_SETS_PATH, 'r') as f:\n",
    "    feature_sets = json.load(f)\n",
    "\n",
    "# Load best model info\n",
    "with open(BEST_MODEL_INFO_PATH, 'r') as f:\n",
    "    best_model_info = json.load(f)\n",
    "\n",
    "print(f\"Best model: {best_model_info['model_name']}\")\n",
    "print(f\"Feature set: {best_model_info['feature_set']}\")\n",
    "print(f\"Metrics: {best_model_info['metrics']}\")\n",
    "\n",
    "# Load the best model\n",
    "best_model_path = os.path.join(MODELS_DIR, f\"{best_model_info['model_name'].lower().replace(' ', '_')}.pkl\")\n",
    "with open(best_model_path, 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "# Get the features and target\n",
    "feature_set_name = best_model_info['feature_set']\n",
    "features = feature_sets[feature_set_name]\n",
    "X = df_engineered[features]\n",
    "y = df_engineered['Exited']\n",
    "\n",
    "# Display the first few rows of the features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coefficient Analysis for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# If the best model is Logistic Regression, analyze coefficients\n",
    "if best_model_info['model_name'] == 'Logistic Regression':\n",
    "    # Get coefficients\n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Coefficient': best_model.coef_[0],\n",
    "        'Abs_Coefficient': np.abs(best_model.coef_[0])\n",
    "    })\n",
    "    \n",
    "    # Sort by absolute coefficient value\n",
    "    coefficients_sorted = coefficients.sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    # Plot coefficients\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x='Coefficient', y='Feature', data=coefficients_sorted)\n",
    "    plt.title('Logistic Regression Coefficients', fontsize=15)\n",
    "    plt.xlabel('Coefficient Value', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.axvline(x=0, color='black', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, 'logistic_regression_coefficients.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print interpretation of top positive and negative coefficients\n",
    "    print(\"Top 5 features increasing churn probability:\")\n",
    "    for i, row in coefficients_sorted[coefficients_sorted['Coefficient'] > 0].head(5).iterrows():\n",
    "        print(f\"{row['Feature']}: {row['Coefficient']:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 5 features decreasing churn probability:\")\n",
    "    for i, row in coefficients_sorted[coefficients_sorted['Coefficient'] < 0].sort_values('Coefficient').head(5).iterrows():\n",
    "        print(f\"{row['Feature']}: {row['Coefficient']:.4f}\")\n",
    "    \n",
    "    # Save coefficients to CSV\n",
    "    coefficients_sorted.to_csv(os.path.join('../docs/', 'logistic_regression_coefficients.csv'), index=False)\n",
    "    print(\"Saved coefficients to docs/logistic_regression_coefficients.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate permutation feature importance\n",
    "perm_importance = permutation_importance(best_model, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create DataFrame with feature importances\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': perm_importance.importances_mean,\n",
    "    'Std': perm_importance.importances_std\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot permutation feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x='Importance', y='Feature', data=perm_importance_df)\n",
    "plt.title('Permutation Feature Importance', fontsize=15)\n",
    "plt.xlabel('Mean Decrease in Accuracy', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOTS_DIR, 'permutation_importance.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print interpretation of top features\n",
    "print(\"Top 10 most important features based on permutation importance:\")\n",
    "for i, row in perm_importance_df.head(10).iterrows():\n",
    "    print(f\"{row['Feature']}: {row['Importance']:.4f} Â± {row['Std']:.4f}\")\n",
    "\n",
    "# Save permutation importance to CSV\n",
    "perm_importance_df.to_csv(os.path.join('../docs/', 'permutation_importance.csv'), index=False)\n",
    "print(\"Saved permutation importance to docs/permutation_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SHAP (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a SHAP explainer\n",
    "if best_model_info['model_name'] in ['Logistic Regression', 'Random Forest', 'XGBoost']:\n",
    "    # For tree-based models or linear models\n",
    "    if best_model_info['model_name'] == 'XGBoost':\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "    elif best_model_info['model_name'] == 'Random Forest':\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "    else:  # Logistic Regression\n",
    "        explainer = shap.LinearExplainer(best_model, X)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    \n",
    "    # If the output is a list (for multi-class), take the values for class 1 (churn)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "    plt.title('SHAP Feature Importance', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, 'shap_importance.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed summary plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values, X, show=False)\n",
    "    plt.title('SHAP Summary Plot', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, 'shap_summary.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create DataFrame with SHAP values\n",
    "    shap_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'SHAP_Importance': np.abs(shap_values).mean(axis=0)\n",
    "    }).sort_values('SHAP_Importance', ascending=False)\n",
    "    \n",
    "    # Print interpretation of top features\n",
    "    print(\"Top 10 most important features based on SHAP values:\")\n",
    "    for i, row in shap_df.head(10).iterrows():\n",
    "        print(f\"{row['Feature']}: {row['SHAP_Importance']:.4f}\")\n",
    "    \n",
    "    # Save SHAP importance to CSV\n",
    "    shap_df.to_csv(os.path.join('../docs/', 'shap_importance.csv'), index=False)\n",
    "    print(\"Saved SHAP importance to docs/shap_importance.csv\")\n",
    "    \n",
    "    # Analyze individual features with highest SHAP values\n",
    "    top_features = shap_df['Feature'].head(5).tolist()\n",
    "    for feature in top_features:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.dependence_plot(feature, shap_values, X, show=False)\n",
    "        plt.title(f'SHAP Dependence Plot for {feature}', fontsize=15)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOTS_DIR, f'shap_dependence_{feature}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "else:\n",
    "    # For other models, use KernelExplainer (slower but model-agnostic)\n",
    "    # Sample a subset of data for KernelExplainer to make it faster\n",
    "    X_sample = shap.sample(X, 100)\n",
    "    explainer = shap.KernelExplainer(best_model.predict_proba, X_sample)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # If the output is a list (for multi-class), take the values for class 1 (churn)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "    plt.title('SHAP Feature Importance (Sample)', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, 'shap_importance.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create DataFrame with SHAP values\n",
    "    shap_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'SHAP_Importance': np.abs(shap_values).mean(axis=0)\n",
    "    }).sort_values('SHAP_Importance', ascending=False)\n",
    "    \n",
    "    # Print interpretation of top features\n",
    "    print(\"Top 10 most important features based on SHAP values (from sample):\")\n",
    "    for i, row in shap_df.head(10).iterrows():\n",
    "        print(f\"{row['Feature']}: {row['SHAP_Importance']:.4f}\")\n",
    "    \n",
    "    # Save SHAP importance to CSV\n",
    "    shap_df.to_csv(os.path.join('../docs/', 'shap_importance.csv'), index=False)\n",
    "    print(\"Saved SHAP importance to docs/shap_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LIME (Local Interpretable Model-agnostic Explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a LIME explainer\n",
    "lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "    X.values,\n",
    "    feature_names=X.columns,\n",
    "    class_names=['Not Churn', 'Churn'],\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Select a few examples to explain\n",
    "# 1. A customer who churned\n",
    "churned_indices = np.where(y == 1)[0]\n",
    "churned_idx = np.random.choice(churned_indices)\n",
    "churned_instance = X.iloc[churned_idx].values\n",
    "\n",
    "# 2. A customer who didn't churn\n",
    "not_churned_indices = np.where(y == 0)[0]\n",
    "not_churned_idx = np.random.choice(not_churned_indices)\n",
    "not_churned_instance = X.iloc[not_churned_idx].values\n",
    "\n",
    "# 3. A customer with high churn probability\n",
    "probs = best_model.predict_proba(X)[:, 1]\n",
    "high_prob_indices = np.argsort(probs)[-10:]  # Top 10 highest probabilities\n",
    "high_prob_idx = np.random.choice(high_prob_indices)\n",
    "high_prob_instance = X.iloc[high_prob_idx].values\n",
    "\n",
    "# Function to explain an instance with LIME\n",
    "def explain_instance(instance, instance_type):\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        instance, \n",
    "        best_model.predict_proba,\n",
    "        num_features=10\n",
    "    )\n",
    "    \n",
    "    # Plot explanation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    explanation.as_pyplot_figure()\n",
    "    plt.title(f'LIME Explanation for {instance_type} Customer', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, f'lime_{instance_type.lower().replace(\" \", \"_\")}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Get explanation as a list of tuples (feature, weight)\n",
    "    feature_weights = explanation.as_list()\n",
    "    print(f\"LIME Explanation for {instance_type} Customer:\")\n",
    "    for feature, weight in feature_weights:\n",
    "        print(f\"{feature}: {weight:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return feature_weights\n",
    "\n",
    "# Explain the selected instances\n",
    "churned_explanation = explain_instance(churned_instance, \"Churned\")\n",
    "not_churned_explanation = explain_instance(not_churned_instance, \"Not Churned\")\n",
    "high_prob_explanation = explain_instance(high_prob_instance, \"High Churn Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Customer Segments Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze churn probability across different customer segments\n",
    "# Make predictions on the entire dataset\n",
    "df_engineered['ChurnProbability'] = best_model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Define key segments based on our EDA insights\n",
    "segments = [\n",
    "    ('Age', ['<30', '30-40', '40-50', '50-60', '>60']),\n",
    "    ('Geography', ['France', 'Germany', 'Spain']),\n",
    "    ('IsActiveMember', ['Inactive', 'Active']),\n",
    "    ('NumOfProducts', [1, 2, 3, 4]),\n",
    "    ('HasHighBalance', ['Low Balance', 'High Balance'])\n",
    "]\n",
    "\n",
    "# Create age groups if not already present\n",
    "if 'AgeGroup_str' not in df_engineered.columns:\n",
    "    df_engineered['AgeGroup_str'] = pd.cut(\n",
    "        df_engineered['Age'], \n",
    "        bins=[0, 30, 40, 50, 60, 100], \n",
    "        labels=['<30', '30-40', '40-50', '50-60', '>60']\n",
    "    )\n",
    "\n",
    "# Create active member labels\n",
    "df_engineered['ActiveMember_str'] = df_engineered['IsActiveMember'].map({0: 'Inactive', 1: 'Active'})\n",
    "\n",
    "# Create balance labels\n",
    "df_engineered['Balance_str'] = df_engineered['HasHighBalance'].map({0: 'Low Balance', 1: 'High Balance'})\n",
    "\n",
    "# Analyze and visualize churn probability by segment\n",
    "
(Content truncated due to size limit. Use line ranges to read in chunks)