{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Model Interpretability\n",
    "\n",
    "This notebook implements model interpretability techniques to understand the factors driving customer churn predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set up plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Create directories for saving outputs\n",
    "os.makedirs('../docs/plots', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the engineered data\n",
    "df_engineered = pd.read_csv('../data/processed/churn_engineered.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df_engineered.shape}\")\n",
    "print(f\"\\nSample data:\")\n",
    "df_engineered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature sets\n",
    "with open('../models/feature_sets.json', 'r') as f:\n",
    "    feature_sets = json.load(f)\n",
    "\n",
    "# Load best model information\n",
    "with open('../models/best_model_info.json', 'r') as f:\n",
    "    best_model_info = json.load(f)\n",
    "\n",
    "print(f\"Best model: {best_model_info['model_name']}\")\n",
    "print(f\"Feature set: {best_model_info['feature_set']}\")\n",
    "print(f\"Metrics: {best_model_info['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model_path = f\"../models/{best_model_info['model_name'].lower().replace(' ', '_')}.pkl\"\n",
    "with open(model_path, 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded model from {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for a specific feature set\n",
    "def prepare_data(df, feature_set_name, test_size=0.2, random_state=42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Get features for the specified feature set\n",
    "    features = feature_sets[feature_set_name]\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df[features]\n",
    "    y = df['Exited']\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for interpretability\n",
    "feature_set_name = best_model_info['feature_set']\n",
    "X_train, X_test, y_train, y_test = prepare_data(df_engineered, feature_set_name)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X_train.columns.tolist()\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coefficient Analysis (for Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the best model is Logistic Regression\n",
    "if best_model_info['model_name'] == 'Logistic Regression':\n",
    "    # Get coefficients\n",
    "    coefficients = best_model.coef_[0]\n",
    "    intercept = best_model.intercept_[0]\n",
    "    \n",
    "    # Create DataFrame with coefficients\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefficients\n",
    "    })\n",
    "    \n",
    "    # Sort by absolute coefficient value\n",
    "    coef_df['Abs_Coefficient'] = coef_df['Coefficient'].abs()\n",
    "    coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Display coefficients\n",
    "    print(f\"Intercept: {intercept:.4f}\")\n",
    "    print(\"\\nTop coefficients:\")\n",
    "    coef_df[['Feature', 'Coefficient']].head(20)\n",
    "else:\n",
    "    print(\"The best model is not Logistic Regression. Skipping coefficient analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficients\n",
    "if best_model_info['model_name'] == 'Logistic Regression':\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot top 15 coefficients\n",
    "    top_coef_df = coef_df.head(15).copy()\n",
    "    top_coef_df['Color'] = top_coef_df['Coefficient'].apply(lambda x: 'red' if x > 0 else 'green')\n",
    "    \n",
    "    # Sort by coefficient value for better visualization\n",
    "    top_coef_df = top_coef_df.sort_values('Coefficient')\n",
    "    \n",
    "    plt.barh(top_coef_df['Feature'], top_coef_df['Coefficient'], color=top_coef_df['Color'])\n",
    "    plt.axvline(x=0, color='black', linestyle='--')\n",
    "    plt.xlabel('Coefficient Value', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.title('Top 15 Logistic Regression Coefficients', fontsize=15)\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../docs/plots/logistic_regression_coefficients.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save coefficients to CSV\n",
    "    coef_df[['Feature', 'Coefficient']].to_csv('../docs/logistic_regression_coefficients.csv', index=False)\n",
    "    print(\"Coefficients saved to ../docs/logistic_regression_coefficients.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importance\n",
    "perm_importance = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Create DataFrame with permutation importance\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': perm_importance.importances_mean,\n",
    "    'Std': perm_importance.importances_std\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "perm_importance_df = perm_importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display permutation importance\n",
    "print(\"Permutation Importance:\")\n",
    "perm_importance_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize permutation importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_perm_importance = perm_importance_df.head(15)\n",
    "\n",
    "# Sort by importance for better visualization\n",
    "top_perm_importance = top_perm_importance.sort_values('Importance')\n",
    "\n",
    "plt.barh(top_perm_importance['Feature'], top_perm_importance['Importance'], \n",
    "         xerr=top_perm_importance['Std'], capsize=5, color='skyblue')\n",
    "plt.xlabel('Permutation Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Top 15 Features by Permutation Importance', fontsize=15)\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/permutation_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save permutation importance to CSV\n",
    "perm_importance_df.to_csv('../docs/permutation_importance.csv', index=False)\n",
    "print(\"Permutation importance saved to ../docs/permutation_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "if best_model_info['model_name'] == 'Logistic Regression':\n",
    "    # For Logistic Regression, use LinearExplainer\n",
    "    explainer = shap.LinearExplainer(best_model, X_train)\n",
    "else:\n",
    "    # For other models, use KernelExplainer\n",
    "    explainer = shap.KernelExplainer(best_model.predict_proba, shap.sample(X_train, 100))\n",
    "\n",
    "# Calculate SHAP values for a sample of test data\n",
    "X_test_sample = X_test.sample(100, random_state=42)\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "# For classification models, shap_values might be a list with values for each class\n",
    "if isinstance(shap_values, list):\n",
    "    # Use values for class 1 (churn)\n",
    "    shap_values = shap_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_test_sample, plot_type=\"bar\", show=False)\n",
    "plt.title('SHAP Feature Importance', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/shap_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot with feature values\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_test_sample, show=False)\n",
    "plt.title('SHAP Summary Plot', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/shap_summary_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plots for top features\n",
    "top_features = perm_importance_df['Feature'].head(5).tolist()\n",
    "\n",
    "for feature in top_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.dependence_plot(feature, shap_values, X_test_sample, show=False)\n",
    "    plt.title(f'SHAP Dependence Plot for {feature}', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../docs/plots/shap_dependence_{feature}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Customer Segment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze churn rates by different customer segments\n",
    "\n",
    "# Age groups\n",
    "df_engineered['AgeGroup'] = pd.cut(df_engineered['Age'], bins=[0, 30, 40, 50, 60, 100], labels=['<30', '30-40', '40-50', '50-60', '>60'])\n",
    "age_group_churn = df_engineered.groupby('AgeGroup')['Exited'].agg(['count', 'mean'])\n",
    "age_group_churn['mean'] = age_group_churn['mean'] * 100  # Convert to percentage\n",
    "age_group_churn.columns = ['Count', 'Churn Rate (%)']  # Rename columns\n",
    "\n",
    "print(\"Churn rate by age group:\")\n",
    "print(age_group_churn)\n",
    "\n",
    "# Geography\n",
    "geography_churn = df_engineered.groupby(['Geography_France', 'Geography_Germany', 'Geography_Spain'])['Exited'].agg(['count', 'mean'])\n",
    "geography_churn['mean'] = geography_churn['mean'] * 100  # Convert to percentage\n",
    "geography_churn.columns = ['Count', 'Churn Rate (%)']  # Rename columns\n",
    "\n",
    "# Create a more readable index\n",
    "geography_mapping = {\n",
    "    (1, 0, 0): 'France',\n",
    "    (0, 1, 0): 'Germany',\n",
    "    (0, 0, 1): 'Spain'\n",
    "}\n",
    "geography_churn = geography_churn.rename(index=geography_mapping)\n",
    "\n",
    "print(\"\\nChurn rate by geography:\")\n",
    "print(geography_churn)\n",
    "\n",
    "# Activity status\n",
    "activity_churn = df_engineered.groupby('IsActiveMember')['Exited'].agg(['count', 'mean'])\n",
    "activity_churn['mean'] = activity_churn['mean'] * 100  # Convert to percentage\n",
    "activity_churn.columns = ['Count', 'Churn Rate (%)']  # Rename columns\n",
    "activity_churn.index = ['Inactive', 'Active']  # Rename index\n",
    "\n",
    "print(\"\\nChurn rate by activity status:\")\n",
    "print(activity_churn)\n",
    "\n",
    "# Number of products\n",
    "product_churn = df_engineered.groupby('NumOfProducts')['Exited'].agg(['count', 'mean'])\n",
    "product_churn['mean'] = product_churn['mean'] * 100  # Convert to percentage\n",
    "product_churn.columns = ['Count', 'Churn Rate (%)']  # Rename columns\n",
    "\n",
    "print(\"\\nChurn rate by number of products:\")\n",
    "print(product_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize churn rates by customer segments\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Age groups\n",
    "age_group_churn['Churn Rate (%)'].plot(kind='bar', ax=axes[0, 0], color='skyblue')\n",
    "axes[0, 0].set_title('Churn Rate by Age Group', fontsize=14)\n",
    "axes[0, 0].set_xlabel('Age Group', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Churn Rate (%)', fontsize=12)\n",
    "axes[0, 0].grid(axis='y')\n",
    "for i, v in enumerate(age_group_churn['Churn Rate (%)']):\n",
    "    axes[0, 0].text(i, v + 1, f\"{v:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "# Geography\n",
    "geography_churn['Churn Rate (%)'].plot(kind='bar', ax=axes[0, 1], color='lightgreen')\n",
    "axes[0, 1].set_title('Churn Rate by Geography', fontsize=14)\n",
    "axes[0, 1].set_xlabel('Geography', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Churn Rate (%)', fontsize=12)\n",
    "axes[0, 1].grid(axis='y')\n",
    "for i, v in enumerate(geography_churn['Churn Rate (%)']):\n",
    "    axes[0, 1].text(i, v + 1, f\"{v:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "# Activity status\n",
    "activity_churn['Churn Rate (%)'].plot(kind='bar', ax=axes[1, 0], color='salmon')\n",
    "axes[1, 0].set_title('Churn Rate by Activity Status', fontsize=14)\n",
    "axes[1, 0].set_xlabel('Activity Status', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Churn Rate (%)', fontsize=12)\n",
    "axes[1, 0].grid(axis='y')\n",
    "for i, v in enumerate(activity_churn['Churn Rate (%)']):\n",
    "    axes[1, 0].text(i, v + 1, f\"{v:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "# Number of products\n",
    "product_churn['Churn Rate (%)'].plot(kind='bar', ax=axes[1, 1], color='mediumpurple')\n",
    "axes[1, 1].set_title('Churn Rate by Number of Products', fontsize=14)\n",
    "axes[1, 1].set_xlabel('Number of Products', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Churn Rate (%)', fontsize=12)\n",
    "axes[1, 1].grid(axis='y')\n",
    "for i, v in enumerate(product_churn['Churn Rate (%)']):\n",
    "    axes[1, 1].text(i, v + 1, f\"{v:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/churn_rate_by_segments.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. High-Risk Customer Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on entire dataset\n",
    "X_full = df_engineered[feature_names]\n",
    "y_pred_proba = best_model.predict_proba(X_full)[:, 1]\n",
    "\n",
    "# Identify high-risk customers (top 10% by probability)\n",
    "high_risk_threshold = np.percentile(y_pred_proba, 90)\n",
    "high_risk_customers = df_engineered[y_pred_proba >= high_risk_threshold]\n",
    "\n",
    "# Analyze characteristics of high-risk customers\n",
    "high_risk_profile = high_risk_customers.describe()\n",
    "overall_profile = df_engineered.describe()\n",
    "\n",
    "# Compare high-risk profile to overall population\n",
    "profile_comparison = pd.concat([high_risk_profile, overall_profile], axis=1, keys=['High Risk', 'Overall'])\n",
    "profile_comparison = profile_comparison.loc['mean']\n",
    "\n",
    "# Visualize high-risk profile\n",
    "plt.figure(figsize=(12, 8))\n",
    "profile_comparison.plot(kind='bar')\n",
    "plt.title('High-Risk vs Overall Customer Profile')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.legend(['High Risk', 'Overall'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/high_risk_profile.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Business Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile key churn factors\n",
    "key_factors = perm_importance_df['Feature'].head(5).tolist()\n",
    "\n",
    "# Identify high-risk segments\n",
    "high_risk_segments = {\n",
    "    'Age': high_risk_customers['Age'].mean(),\n",
    "    'Geography': high_risk_customers['Geography'].mode().values[0],\n",
    "    'IsActiveMember': high_risk_customers['IsActiveMember'].mean(),\n",
    "    'NumOfProducts': high_risk_customers['NumOfProducts'].mean()\n",
    "}\n",
    "\n",
    "# Provide business recommendations\n",
    "recommendations = [\n",
    "    \"Focus retention efforts on customers with profiles similar to the high-risk segment\",\n",
    "    f\"Pay special attention to customers in {high_risk_segments['Geography']}\",\n",
    "    \"Develop targeted retention strategies for inactive members\",\n",
    "    \"Consider offering product bundles to increase the number of products per customer\",\n",
    "    \"Implement a proactive outreach program for customers as they approach the average high-risk age\"\n",
    "]\n",
    "\n",
    "# Save insights to JSON\n",
    "insights = {\n",
    "    'key_churn_factors': key_factors,\n",
    "    'high_risk_segments': high_risk_segments,\n",
    "    'recommendations': recommendations\n",
    "}\n",
    "\n",
    "with open('../docs/business_insights.json', 'w') as f:\n",
    "    json.dump(insights, f, indent=4)\n",
    "\n",
    "print(\"Business insights and recommendations saved to ../docs/business_insights.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Customer Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_customer(customer_data):\n",
    "    # Make prediction\n",
    "    prediction = best_model.predict_proba(customer_data.reshape(1, -1))[0, 1]\n",
    "    \n",
    "    # Get SHAP values\n",
    "    explainer = shap.Explainer(best_model, X_train)\n",
    "    shap_values = explainer(customer_data.reshape(1, -1))\n",
    "    \n",
    "    # Identify top risk factors\n",
    "    risk_factors = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': np.abs(shap_values.values[0])\n",
    "    }).sort_values('importance', ascending=False).head(3)\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = []\n",
    "    for _, factor in risk_factors.iterrows():\n",
    "        if factor['importance'] > 0:\n",
    "            if 'Age' in factor['feature']:\n",
    "                recommendations.append(\"Consider age-specific retention offers\")\n",
    "            elif 'IsActiveMember' in factor['feature']:\n",
    "                recommendations.append(\"Encourage more active engagement with our services\")\n",
    "            elif 'NumOfProducts' in factor['feature']:\n",
    "                recommendations.append(\"Offer additional products that complement current usage\")\n",
    "    \n",
    "    return {\n",
    "        'churn_probability': prediction,\n",
    "        'risk_factors': risk_factors.to_dict('records'),\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "\n",
    "# Analyze sample high-risk and low-risk customers\n",
    "high_risk_sample = X_test.iloc[y_pred_proba[X_test.index].argmax()]\n",
    "low_risk_sample = X_test.iloc[y_pred_proba[X_test.index].argmin()]\n",
    "\n",
    "print(\"High-risk customer analysis:\")\n",
    "print(json.dumps(analyze_customer(high_risk_sample), indent=2))\n",
    "\n",
    "print(\"\\nLow-risk customer analysis:\")\n",
    "print(json.dumps(analyze_customer(low_risk_sample), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
