{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Model Development\n",
    "\n",
    "This notebook implements and evaluates various machine learning models for predicting customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Set up plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Create directories for saving outputs\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../docs/plots', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the engineered data\n",
    "df_engineered = pd.read_csv('../data/processed/churn_engineered.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df_engineered.shape}\")\n",
    "print(f\"\\nColumns: {df_engineered.columns.tolist()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "df_engineered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature sets\n",
    "with open('../models/feature_sets.json', 'r') as f:\n",
    "    feature_sets = json.load(f)\n",
    "\n",
    "# Print feature set sizes\n",
    "for name, features in feature_sets.items():\n",
    "    print(f\"{name}: {len(features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for a specific feature set\n",
    "def prepare_data(df, feature_set_name, test_size=0.2, random_state=42, apply_smote=False):\n",
    "    # Get features for the specified feature set\n",
    "    features = feature_sets[feature_set_name]\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df[features]\n",
    "    y = df['Exited']\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    \n",
    "    # Apply SMOTE to handle class imbalance if requested\n",
    "    if apply_smote:\n",
    "        smote = SMOTE(random_state=random_state)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(f\"After SMOTE - Class distribution: {pd.Series(y_train).value_counts()}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(df_engineered['Exited'].value_counts())\n",
    "print(f\"Churn rate: {df_engineered['Exited'].mean() * 100:.2f}%\")\n",
    "\n",
    "# Prepare data for modeling using the selected feature set\n",
    "feature_set_name = 'selected_top'  # We'll use the selected top features\n",
    "X_train, X_test, y_train, y_test = prepare_data(df_engineered, feature_set_name, apply_smote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_prob_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_lr):.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/lr_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Save the model\n",
    "with open('../models/logistic_regression.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "print(\"Logistic Regression model saved to ../models/logistic_regression.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_rf):.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/rf_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "plt.title('Random Forest Feature Importance', fontsize=15)\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/rf_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "with open('../models/random_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "print(\"Random Forest model saved to ../models/random_forest.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, scale_pos_weight=4)  # scale_pos_weight helps with class imbalance\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_xgb):.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - XGBoost')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/xgb_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "plt.title('XGBoost Feature Importance', fontsize=15)\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/xgb_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "with open('../models/xgboost.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "print(\"XGBoost model saved to ../models/xgboost.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM model\n",
    "svm_model = SVC(random_state=42, probability=True, class_weight='balanced')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "y_prob_svm = svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"SVM Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_svm):.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - SVM')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/svm_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Save the model\n",
    "with open('../models/svm.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model, f)\n",
    "print(\"SVM model saved to ../models/svm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Neural Network model\n",
    "nn_model = MLPClassifier(random_state=42, max_iter=1000, hidden_layer_sizes=(100, 50), alpha=0.01)\n",
    "nn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nn = nn_model.predict(X_test)\n",
    "y_prob_nn = nn_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Neural Network Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_nn):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_nn):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_nn):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_nn):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_nn):.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_nn = confusion_matrix(y_test, y_pred_nn)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Neural Network')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/nn_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "\n",
    "# Save the model\n",
    "with open('../models/neural_network.pkl', 'wb') as f:\n",
    "    pickle.dump(nn_model, f)\n",
    "print(\"Neural Network model saved to ../models/neural_network.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect model results\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'y_pred': y_pred_lr,\n",
    "        'y_prob': y_prob_lr,\n",
    "        'metrics': {\n",
    "            'accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "            'precision': precision_score(y_test, y_pred_lr),\n",
    "            'recall': recall_score(y_test, y_pred_lr),\n",
    "            'f1': f1_score(y_test, y_pred_lr),\n",
    "            'roc_auc': roc_auc_score(y_test, y_prob_lr)\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'y_pred': y_pred_rf,\n",
    "        'y_prob': y_prob_rf,\n",
    "        'metrics': {\n",
    "            'accuracy': accuracy_score(y_test, y_pred_rf),\n",
    "            'precision': precision_score(y_test, y_pred_rf),\n",
    "            'recall': recall_score(y_test, y_pred_rf),\n",
    "            'f1': f1_score(y_test, y_pred_rf),\n",
    "            'roc_auc': roc_auc_score(y_test, y_prob_rf)\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'y_pred': y_pred_xgb,\n",
    "        'y_prob': y_prob_xgb,\n",
    "        'metrics': {\n",
    "            'accuracy': accuracy_score(y_test, y_pred_xgb),\n",
    "            'precision': precision_score(y_test, y_pred_xgb),\n",
    "            'recall': recall_score(y_test, y_pred_xgb),\n",
    "            'f1': f1_score(y_test, y_pred_xgb),\n",
    "            'roc_auc': roc_auc_score(y_test, y_prob_xgb)\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'y_pred': y_pred_svm,\n",
    "        'y_prob': y_prob_svm,\n",
    "        'metrics': {\n",
    "            'accuracy': accuracy_score(y_test, y_pred_svm),\n",
    "            'precision': precision_score(y_test, y_pred_svm),\n",
    "            'recall': recall_score(y_test, y_pred_svm),\n",
    "            'f1': f1_score(y_test, y_pred_svm),\n",
    "            'roc_auc': roc_auc_score(y_test, y_prob_svm)\n",
    "        }\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'y_pred': y_pred_nn,\n",
    "        'y_prob': y_prob_nn,\n",
    "        'metrics': {\n",
    "            'accuracy': accuracy_score(y_test, y_pred_nn),\n",
    "            'precision': precision_score(y_test, y_pred_nn),\n",
    "            'recall': recall_score(y_test, y_pred_nn),\n",
    "            'f1': f1_score(y_test, y_pred_nn),\n",
    "            'roc_auc': roc_auc_score(y_test, y_prob_nn)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a DataFrame for model comparison\n",
    "metrics_df = pd.DataFrame({\n",
    "    model_name: {metric: value for metric, value in model_info['metrics'].items()}\n",
    "    for model_name, model_info in models.items()\n",
    "}).T\n",
    "\n",
    "# Format metrics for better readability\n",
    "metrics_df = metrics_df.round(4)\n",
    "\n",
    "# Display model comparison\n",
    "print(\"Model Comparison:\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot metrics comparison\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.barplot(x=metrics_df.index, y=metrics_df[metric], palette=colors)\n",
    "    plt.title(f'{metric.upper()} Comparison', fontsize=14)\n",
    "    plt.ylabel(metric.upper(), fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, model_info['y_prob'])\n",
    "    roc_auc = model_info['metrics']['roc_auc']\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves Comparison', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../docs/plots/roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best model based on ROC-AUC\n",
    "best_model_name = metrics_df['roc_auc'].idxmax()\n",
    "best_model_auc = metrics_df.loc[best_model_name, 'roc_auc']\n",
    "best_model_info = models[best_model_name]\n",
    "\n",
    "print(f\"Best model: {best_model_name} with ROC-AUC: {best_model_auc:.4f}\")\n",
    "\n",
    "# Save best model information\n",
    "best_model_info_dict = {\n",
    "    'model_name': best_model_name,\n",
    "    'metrics': best_model_info['metrics'],\n",
    "    'feature_set': feature_set_name\n",
    "}\n",
    "\n",
    "with open('../models/best_model_info.json', 'w') as f:\n",
    "    json.dump(best_model_info_dict, f)\n",
    "\n",
    "print(f\"Best model information saved to ../models/best_model_info.json\")\n",
    "\n",
    "# Visualize confusion matrix for best model\n",
    "cm_best = confusion_matrix(y_test, best_model_info['y_pred'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix - {best_model_name} (Best Model)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/best_model_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost-Benefit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cost assumptions\n",
    "cost_losing_customer = 1000  # Cost of losing a customer ($)\n",
    "cost_retention_program = 100  # Cost of retention program per customer ($)\n",
    "\n",
    "# Function to calculate cost-benefit\n",
    "def calculate_cost_benefit(y_true, y_pred, y_prob=None, threshold=0.5):\n",
    "    if y_prob is not None:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix values\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate costs\n",
    "    cost_false_negatives = fn * cost_losing_customer  # Missed churn predictions\n",
    "    cost_false_positives = fp * cost_retention_program  # Unnecessary retention actions\n",
    "    cost_true_positives = tp * cost_retention_program  # Necessary retention actions\n",
    "    \n",
    "    # Calculate benefits\n",
    "    benefit_true_positives = tp * cost_losing_customer  # Saved customers\n",
    "    \n",
    "    # Calculate net benefit\n",
    "    total_cost = cost_false_negatives + cost_false_positives + cost_true_positives\n",
    "    total_benefit = benefit_true_positives\n",
    "    net_benefit = total_benefit - total_cost\n",
    "    \n",
    "    return {\n",
    "        'total_cost': total_cost,\n",
    "        'total_benefit': total_benefit,\n",
    "        'net_benefit': net_benefit,\n",
    "        'roi': (total_benefit / total_cost if total_cost > 0 else 0) - 1,\n",
    "        'confusion_matrix': {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}\n",
    "    }\n",
    "\n",
    "# Calculate cost-benefit for each model\n",
    "cost_benefit_results = {}\n",
    "for model_name, model_info in models.items():\n",
    "    cost_benefit_results[model_name] = calculate_cost_benefit(y_test, model_info['y_pred'])\n",
    "\n",
    "# Create DataFrame for cost-benefit comparison\n",
    "cost_benefit_df = pd.DataFrame({\n",
    "    model_name: {\n",
    "        'Total Cost ($)': results['total_cost'],\n",
    "        'Total Benefit ($)': results['total_benefit'],\n",
    "        'Net Benefit ($)': results['net_benefit'],\n",
    "        'ROI (%)': results['roi'] * 100\n",
    "    }\n",
    "    for model_name, results in cost_benefit_results.items()\n",
    "}).T\n",
    "\n",
    "# Format for better readability\n",
    "cost_benefit_df['Total Cost ($)'] = cost_benefit_df['Total Cost ($)'].map('${:,.0f}'.format)\n",
    "cost_benefit_df['Total Benefit ($)'] = cost_benefit_df['Total Benefit ($)'].map('${:,.0f}'.format)\n",
    "cost_benefit_df['Net Benefit ($)'] = cost_benefit_df['Net Benefit ($)'].map('${:,.0f}'.format)\n",
    "cost_benefit_df['ROI (%)'] = cost_benefit_df['ROI (%)'].map('{:.2f}%'.format)\n",
    "\n",
    "# Display cost-benefit comparison\n",
    "print(\"Cost-Benefit Analysis:\")\n",
    "cost_benefit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cost-benefit comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Extract numeric values for plotting\n",
    "net_benefits = [results['net_benefit'] for results in cost_benefit_results.values()]\n",
    "model_names = list(cost_benefit_results.keys())\n",
    "\n",
    "# Create bar chart\n",
    "sns.barplot(x=model_names, y=net_benefits)\n",
    "plt.title('Net Benefit Comparison', fontsize=15)\n",
    "plt.ylabel('Net Benefit ($)', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(net_benefits):\n",
    "    plt.text(i, v + 1000, f'${v:,.0f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/cost_benefit_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze different probability thresholds for the best model\n",
    "thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "threshold_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Get predictions using the threshold\n",
    "    y_pred_threshold = (best_model_info['y_prob'] >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "    precision = precision_score(y_test, y_pred_threshold)\n",
    "    recall = recall_score(y_test, y_pred_threshold)\n",
    "    f1 = f1_score(y_test, y_pred_threshold)\n",
    "    \n",
    "    # Calculate cost-benefit\n",
    "    cost_benefit = calculate_cost_benefit(y_test, None, best_model_info['y_prob'], threshold)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'threshold': threshold,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'net_benefit': cost_benefit['net_benefit']\n",
    "    })\n",
    "\n",
    "# Create DataFrame for threshold analysis\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "# Display threshold analysis\n",
    "print(\"Threshold Analysis:\")\n",
    "threshold_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize threshold analysis\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot metrics vs threshold\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(threshold_df['threshold'], threshold_df['accuracy'], 'b-', label='Accuracy')\n",
    "plt.plot(threshold_df['threshold'], threshold_df['precision'], 'g-', label='Precision')\n",
    "plt.plot(threshold_df['threshold'], threshold_df['recall'], 'r-', label='Recall')\n",
    "plt.plot(threshold_df['threshold'], threshold_df['f1'], 'y-', label='F1 Score')\n",
    "plt.xlabel('Threshold', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Metrics vs Threshold', fontsize=15)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Plot net benefit vs threshold\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(threshold_df['threshold'], threshold_df['net_benefit'], 'b-')\n",
    "plt.xlabel('Threshold', fontsize=12)\n",
    "plt.ylabel('Net Benefit ($)', fontsize=12)\n",
    "plt.title('Net Benefit vs Threshold', fontsize=15)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Find optimal threshold based on net benefit\n",
    "optimal_threshold_idx = threshold_df['net_benefit'].idxmax()\n",
    "optimal_threshold = threshold_df.loc[optimal_threshold_idx, 'threshold']\n",
    "optimal_net_benefit = threshold_df.loc[optimal_threshold_idx, 'net_benefit']\n",
    "\n",
    "plt.axvline(x=optimal_threshold, color='r', linestyle='--')\n",
    "plt.text(optimal_threshold + 0.02, optimal_net_benefit - 5000, \n",
    "         f'Optimal Threshold: {optimal_threshold:.2f}\\nNet Benefit: ${optimal_net_benefit:,.0f}',\n",
    "         fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/plots/threshold_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold:.2f} with net benefit: ${optimal_net_benefit:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model comparison results\n",
    "metrics_df.to_csv('../models/model_comparison.csv')\n",
    "print(\"Model comparison results saved to ../models/model_comparison.csv\")\n",
    "\n",
    "# Save cost-benefit analysis results\n",
    "pd.DataFrame(cost_benefit_results).to_json('../models/cost_benefit_results.json')\n",
    "print(\"Cost-benefit analysis results saved to ../models/cost_benefit_results.json\")\n",
    "\n",
    "# Save threshold analysis results\n",
    "threshold_df.to_csv('../models/threshold_analysis.csv')\n",
    "print(\"Threshold analysis results saved to ../models/threshold_analysis.csv\")\n",
    "\n",
    "# Save optimal threshold\n",
    "optimal_threshold_info = {\n",
    "    'optimal_threshold': float(optimal_threshold),\n",
    "    'metrics_at_optimal_threshold': {\n",
    "        'accuracy': float(threshold_df.loc[optimal_threshold_idx, 'accuracy']),\n",
    "        'precision': float(threshold_df.loc[optimal_threshold_idx, 'precision']),\n",
    "        'recall': float(threshold_df.loc[optimal_threshold_idx, 'recall']),\n",
    "        'f1': float(threshold_df.loc[optimal_threshold_idx, 'f1']),\n",
    "        'net_benefit': float(optimal_net_benefit)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../models/optimal_threshold.json', 'w') as f:\n",
    "    json.dump(optimal_threshold_info, f)\n",
    "print(\"Optimal threshold information saved to ../models/optimal_threshold.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\n=== Customer Churn Prediction Model Development Summary ===\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"ROC-AUC: {best_model_auc:.4f}\")\n",
    "print(f\"Optimal threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"Net benefit at optimal threshold: ${optimal_net_benefit:,.0f}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Deploy the model in production\")\n",
    "print(\"2. Monitor model performance\")\n",
    "print(\"3. Update the model periodically with new data\")\n",
    "print(\"4. Implement targeted retention strategies based on model predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
